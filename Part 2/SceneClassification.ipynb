{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import json\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchviz in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: torch in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchviz) (2.4.1+cu124)\n",
      "Requirement already satisfied: graphviz in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchviz) (0.20.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch->torchviz) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch->torchviz) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch->torchviz) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch->torchviz) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch->torchviz) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch->torchviz) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch->torchviz) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy->torch->torchviz) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this implementation was inspired by the following source: https://www.digitalocean.com/community/tutorials/alexnet-pytorch\n",
    "class SceneClassificationCNN(nn.Module):\n",
    "    def __init__(self, num_classes=15):\n",
    "        super(SceneClassificationCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ELU(),  \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),  \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ELU())  \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ELU())  \n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),  \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(6400, 4096),\n",
    "            nn.ELU())  \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ELU()) \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(4096, num_classes) \n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='linear')  \n",
    "                m.weight.data *= 1.0  \n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='linear')  \n",
    "                m.weight.data *= 1.0  \n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "def prepare_data_splits(dataset_dir, output_dir, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training, validation, and test sets, and copies them to an output directory,\n",
    "    while balancing the dataset across classes.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_dir: The directory containing class subdirectories of images.\n",
    "    - output_dir: The directory where train, val, and test subdirectories will be created.\n",
    "    - train_ratio: Ratio of data to use for training (default is 0.8).\n",
    "    - val_ratio: Ratio of data to use for validation (default is 0.1).\n",
    "    - test_ratio: Ratio of data to use for testing (default is 0.1).\n",
    "    \"\"\"\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Train, val, and test ratios must sum to 1.\"\n",
    "    \n",
    "    # Create the output directories for splits\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_dir = os.path.join(output_dir, split)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "    class_counts = {}  # To store counts of images per class\n",
    "\n",
    "    # Loop over each class directory in the input dataset\n",
    "    for class_folder in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_folder)\n",
    "        \n",
    "        # Skip if this is not a class directory (e.g., skip hidden files)\n",
    "        if os.path.isdir(class_path):\n",
    "            images = os.listdir(class_path)\n",
    "            class_counts[class_folder] = len(images)  # Count images in this class\n",
    "\n",
    "    # Determine the minimum number of images in any class for balancing\n",
    "    min_class_count = min(class_counts.values())\n",
    "    \n",
    "    # Loop again to create balanced splits\n",
    "    for class_folder in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_folder)\n",
    "        \n",
    "        if os.path.isdir(class_path):\n",
    "            images = os.listdir(class_path)\n",
    "            # Balance by sampling the minimum count\n",
    "            balanced_train_imgs = images[:min_class_count]  # Take the first 'min_class_count' images\n",
    "            \n",
    "            # Split the remaining images into validation and test\n",
    "            remaining_imgs = images[min_class_count:]\n",
    "            train_imgs, temp_imgs = train_test_split(balanced_train_imgs, test_size=(1 - train_ratio))\n",
    "            val_imgs, test_imgs = train_test_split(temp_imgs, test_size=test_ratio / (test_ratio + val_ratio))\n",
    "\n",
    "            # Function to copy images to the corresponding split folder in the output directory\n",
    "            def copy_images(img_list, split_type):\n",
    "                split_class_dir = os.path.join(output_dir, split_type, class_folder)\n",
    "                os.makedirs(split_class_dir, exist_ok=True)\n",
    "                for img in img_list:\n",
    "                    src_img_path = os.path.join(class_path, img)\n",
    "                    dest_img_path = os.path.join(split_class_dir, img)\n",
    "                    shutil.copy(src_img_path, dest_img_path)\n",
    "\n",
    "            # Copy images to the respective directories in the output folder\n",
    "            copy_images(train_imgs, 'train')\n",
    "            copy_images(val_imgs, 'val')\n",
    "            copy_images(test_imgs, 'test')\n",
    "\n",
    "    print(f\"Data successfully split and copied into train, val, and test sets in '{output_dir}'.\")\n",
    "\n",
    "# Example usage\n",
    "prepare_data_splits('data/15-Scene', 'data/preprocessed-data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images:\n",
      "Train dataset: 2520\n",
      "Validation dataset: 315\n",
      "Test dataset: 315\n"
     ]
    }
   ],
   "source": [
    "# Define data augmentation and transformation\n",
    "from collections import Counter\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder('data/preprocessed-data/train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder('data/preprocessed-data/val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder('data/preprocessed-data/test', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "# def print_class_distribution(dataset, dataset_name):\n",
    "#     class_counts = Counter([label for _, label in dataset.samples])\n",
    "#     print(f\"\\n{dataset_name} dataset distribution:\")\n",
    "#     for class_name, class_idx in dataset.class_to_idx.items():\n",
    "#         print(f\"{class_name}: {class_counts[class_idx]} images\")\n",
    "\n",
    "# Print total number of images\n",
    "print('Total number of images:')\n",
    "print('Train dataset:', len(train_dataset))\n",
    "print('Validation dataset:', len(val_dataset))\n",
    "print('Test dataset:', len(test_dataset))\n",
    "\n",
    "# Print number of images per class\n",
    "# print_class_distribution(train_dataset, 'Train')\n",
    "# print_class_distribution(val_dataset, 'Validation')\n",
    "# print_class_distribution(test_dataset, 'Test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, optimizer_type, lr=0.001):\n",
    "    if optimizer_type == 'SGD':\n",
    "        return optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    elif optimizer_type == 'Adam':\n",
    "        return optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_type == 'RMSProp':\n",
    "        return optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer type: {optimizer_type}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SceneClassificationCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, test_loader, optimizer,scheduler, num_epochs): \n",
    "    # To store training data for plotting\n",
    "    training_data = {\n",
    "        'train_loss': [],\n",
    "        'test_loss': [],\n",
    "        'train_error': [],\n",
    "        'test_error': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': []\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        rain_loader_iter = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for images, labels in rain_loader_iter:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        scheduler.step()\n",
    "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        train_error = 1 - (correct_train / total_train)\n",
    "\n",
    "        # Testing phase\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        test_loader_iter = tqdm(test_loader, desc=f\"Validating Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader_iter:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_test_loss += loss.item() * images.size(0)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "        test_error = 1 - (correct_test / total_test)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "\n",
    "        # Store metrics\n",
    "        training_data['train_loss'].append(avg_train_loss)\n",
    "        training_data['test_loss'].append(avg_test_loss)\n",
    "        training_data['train_error'].append(train_error)\n",
    "        training_data['test_error'].append(test_error)\n",
    "        training_data['accuracy'].append(accuracy)\n",
    "        training_data['precision'].append(precision)\n",
    "        training_data['recall'].append(recall)\n",
    "        training_data['f1_score'].append(f1)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}, '\n",
    "              f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, '\n",
    "              f'Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    training_data['training_time'] = training_time\n",
    "\n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_results(model, model_name, training_data):\n",
    "    torch.save(model.state_dict(), f'{model_name}_model.pth')\n",
    "    # Convert training data to a JSON-serializable format\n",
    "    serializable_data = {}\n",
    "    for key, value in training_data.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            serializable_data[key] = value.tolist()  # Convert Tensors to lists\n",
    "        elif isinstance(value, list) and isinstance(value[0], torch.Tensor):\n",
    "            serializable_data[key] = [v.tolist() for v in value]  # Convert lists of Tensors\n",
    "        else:\n",
    "            serializable_data[key] = value\n",
    "\n",
    "    # Save the training data as JSON\n",
    "    with open(f'{model_name}_training_data.json', 'w') as json_file:\n",
    "        json.dump(serializable_data, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_model(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = ['SGD', 'Adam', 'RMSProp']\n",
    "\n",
    "# Save the initial state of the model\n",
    "initial_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "for opt_type in optimizers:\n",
    "    # Reload the initial state of the model for each optimizer\n",
    "    model.load_state_dict(initial_model_state)\n",
    "\n",
    "    # Get optimizer\n",
    "    optimizer = get_optimizer(model, opt_type)\n",
    "    print(f\"\\nTraining with {opt_type} optimizer:\")\n",
    "\n",
    "    # Scheduler for the optimizer\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0.001)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    training_data = train_and_evaluate(model, train_loader, test_loader, optimizer, scheduler, num_epochs=300)\n",
    "    save_model_and_results(model, f'scene_classification_{opt_type}', training_data)\n",
    "    # Save the trained model and its results\n",
    "    accuracy = evaluate_model(model, val_loader)\n",
    "    print(f'Accuracy for Network on the whole validation set: {accuracy:.2%}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
